import openai
import os
import time
import logging
import shelve
import asyncio
from dotenv import load_dotenv
import sys
sys.stdout.reconfigure(encoding='utf-8')


# âœ… Ä®keliame API raktÄ… iÅ¡ .env failo
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… Sukuriamas OpenAI klientas
client = openai.AsyncOpenAI(api_key=OPENAI_API_KEY)

# âœ… Sukuriami aplankai, jei jÅ³ nÄ—ra
LOG_DIR = "logs"
CACHE_DIR = "cache"

def ensure_cache_directory():
    """UÅ¾tikrina, kad `cache/` katalogas egzistuoja prieÅ¡ naudojant cache failÄ…."""
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)
        print("ğŸ› ï¸ Sukurtas `cache/` katalogas.")

ensure_cache_directory()

# âœ… Nustatoma logÅ³ sistema
LOG_FILE = os.path.join(LOG_DIR, "openai_client.log")
logging.basicConfig(filename=LOG_FILE, level=logging.ERROR,
                    format="%(asctime)s - %(levelname)s - %(message)s")

# âœ… Cache failas
CACHE_FILE = os.path.join(CACHE_DIR, "openai_cache.db")

async def get_current_model():
    """UÅ¾klausia OpenAI API ir grÄ…Å¾ina tikrÄ… GPT modelio versijÄ…, kuri realiai naudojama."""
    ensure_cache_directory()
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",  # Naudojame paskutinÄ¯ patvirtintÄ… modelÄ¯, nes uÅ¾klausa be `model` nesuveiks
            messages=[{"role": "system", "content": "GrÄ…Å¾ink tik modelio versijos pavadinimÄ…."}]
        )
        return response.model  # OpenAI API grÄ…Å¾ina `model` laukÄ… su tiksliu modelio pavadinimu
    except Exception as e:
        print(f"âš ï¸ Klaida gaunant realÅ³ modelio pavadinimÄ…: {e}")
        return "NeÅ¾inoma versija"

async def ask_openai(prompt, max_retries=5):
    """AsinchroniÅ¡kai siunÄia uÅ¾klausÄ… OpenAI API su caching ir backoff retry mechanizmu."""
    ensure_cache_directory()
    retries = 0
    wait_time = 1  # Pradinis laukimo laikas sekundÄ—mis

    try:
        with shelve.open(CACHE_FILE) as cache:
            if prompt in cache:
                print("ğŸ’¾ Atsakymas paimtas iÅ¡ cache!")
                return cache[prompt]

            while retries < max_retries:
                try:
                    response = await client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{"role": "user", "content": prompt}]
                    )
                    result = response.choices[0].message.content.encode("utf-8", errors="replace").decode("utf-8")
                    cache[prompt] = result
                    return result

                except openai.RateLimitError:
                    print(f"âš ï¸ API pasiekÄ— uÅ¾klausÅ³ ribÄ…. Laukiama {wait_time} sek. ir bandoma dar kartÄ…...")
                    await asyncio.sleep(wait_time)
                    wait_time *= 2
                    retries += 1
                
                except openai.APITimeoutError:
                    print(f"â³ API atsako per ilgai. Laukiama {wait_time} sek. ir bandoma dar kartÄ…...")
                    await asyncio.sleep(wait_time)
                    wait_time *= 2
                    retries += 1

                except openai.OpenAIError as e:
                    print(f"âš ï¸ OpenAI API klaida: {e}")
                    return None

            print("ğŸš« Nepavyko gauti atsakymo po keliÅ³ bandymÅ³.")
            return None

    except Exception:
        print("âš ï¸ Cache failas sugadintas. Jis bus iÅ¡trintas ir atkurtas.")
        return None
